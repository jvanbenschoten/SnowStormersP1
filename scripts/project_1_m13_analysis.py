# -*- coding: utf-8 -*-
"""Project#1_M13_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_71KgkxGCpoWFVNjXmBQ7uR1AuntBedI
"""

# Project: DS 4002 – Snow Stormers
# Authors: Aleeza Sadiq, Austin Blackburn, Jacob VanBenschoten
# Date: Spring 2026

# Purpose:
# This script evaluates the predictive accuracy of a pretrained
# HuggingFace RoBERTa sentiment model on Letterboxd movie reviews.
# It generates sentiment predictions, engineers text features,
# computes classification metrics, and analyzes model accuracy
# across linguistic characteristics.

# Inputs:
# - letterboxd_top250_reviews_clean.csv

# Outputs:
# - full_feature_engineering_table.csv
# - model_performance.csv
# - confusion_matrix.csv
# - confusion_matrix.png
# - accuracy_by_language.csv
# - accuracy_by_emoji.csv
# - accuracy_by_length.csv
# - accuracy_by_caps_ratio.csv
# - accuracy_by_low_text_content.csv

!pip install emoji
!pip install langdetect

# To Run:
# python 03_model_analysis.py

import pandas as pd
from transformers import pipeline
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from tqdm import tqdm
import re
import emoji
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException
import seaborn as sns
import matplotlib.pyplot as plt

# Load your cleaned dataset CSV
df = pd.read_csv("letterboxd_top250_reviews_clean.csv")

# Remove empty reviews (prevents rare tokenizer crashes)
df = df[df["review_text"].astype(str).str.strip().str.len() > 0].copy()

# --- Step A: Convert star_rating -> true sentiment category (preregistered bins) ---
def stars_to_label(stars: float) -> str:
    #  Converts numeric star ratings (0.5–5.0) into
    #three sentiment categories based on preregistered bins:
    #<2.5 = negative
    #2.5–3.5 = neutral
    #>3.5 = positive
    if stars < 2.5:
        return "negative"
    elif stars <= 3.5:
        return "neutral"
    else:
        return "positive"

df["true_label"] = df["star_rating"].apply(stars_to_label)

# --- Step B: Load HuggingFace sentiment model (LOCAL; no token needed) ---
model_id = "cardiffnlp/twitter-roberta-base-sentiment-latest"
sentiment_pipe = pipeline("sentiment-analysis", model=model_id, tokenizer=model_id)

# --- Step C: Run predictions in batches (fast + safe) ---
texts = df["review_text"].astype(str).tolist()

batch_size = 32
pred_labels = []
pred_scores = []

for i in tqdm(range(0, len(texts), batch_size), desc="Running HF sentiment"):
    batch = texts[i:i+batch_size]
    preds = sentiment_pipe(batch, truncation=True, max_length=512) # truncation avoids very long text issues
    pred_labels.extend([p["label"].lower() for p in preds])   # labels: 'positive', 'neutral', 'negative'
    pred_scores.extend([p["score"] for p in preds])

df["pred_label"] = pred_labels
df["pred_score"] = pred_scores

# --- Step D: Optional neutral confidence threshold (0.4–0.6) ---
# If confidence is near the boundary, force neutral (TA request)
def apply_neutral_threshold(label, score, low=0.4, high=0.6):
    if low <= score <= high:
        return "neutral"
    return label

df["pred_label_thresh"] = [
    apply_neutral_threshold(l, s, 0.4, 0.6) for l, s in zip(df["pred_label"], df["pred_score"])
]

# --- Step E: Evaluate ---
y_true = df["true_label"]
y_pred = df["pred_label_thresh"]

df["correct"] = (df["true_label"] == df["pred_label_thresh"])

print("Accuracy:", accuracy_score(y_true, y_pred))
print("F1 (macro):", f1_score(y_true, y_pred, average="macro"))
print("\nConfusion Matrix:\n", confusion_matrix(y_true, y_pred, labels=["negative", "neutral", "positive"]))
print("\nClassification Report:\n", classification_report(y_true, y_pred))

print(df["true_label"].value_counts(normalize=True))

df["review_len"] = df["review_text"].str.len()

labels = ["negative", "neutral", "positive"]

cm = confusion_matrix(y_true, y_pred, labels=labels)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=labels,
            yticklabels=labels)

plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix: HF Sentiment vs Star Ratings")
plt.show()

df["pred_label_thresh"].value_counts().plot(kind="bar")
plt.title("Distribution of HF Predicted Sentiment")
plt.xlabel("Predicted Sentiment")
plt.ylabel("Count")
plt.show()

df.boxplot(column="star_rating", by="pred_label_thresh")
plt.title("Star Ratings by HF Predicted Sentiment")
plt.suptitle("")
plt.xlabel("Predicted Sentiment")
plt.ylabel("Star Rating")
plt.show()

df["pred_score"].plot(kind="hist", bins=30)
plt.title("Distribution of HF Confidence Scores")
plt.xlabel("Confidence Score")
plt.ylabel("Count")
plt.show()

df["correct"] = df["true_label"] == df["pred_label_thresh"]

df.boxplot(column="pred_score", by="correct")
plt.title("Model Confidence: Correct vs Incorrect Predictions")
plt.suptitle("")
plt.xlabel("Prediction Correct?")
plt.ylabel("Confidence Score")
plt.show()

df["true_label"].value_counts().plot(kind="bar")
plt.title("True Sentiment Distribution (From Ratings)")
plt.show()

# --- Feature Engineering ---

# 1) Review character length
df["review_char_len"] = df["review_text"].astype(str).str.len()

# 2) Review word count
df["review_word_count"] = df["review_text"].astype(str).str.split().str.len()

# 3) Emoji count
def count_emojis(text):
    text = str(text)
    return sum(1 for ch in text if ch in emoji.EMOJI_DATA)

df["emoji_count"] = df["review_text"].apply(count_emojis)
df["has_emoji"] = df["emoji_count"] > 0

# 4) Capitalization ratio (uppercase letters / total letters)
def caps_ratio(text):
    text = str(text)
    letters = [c for c in text if c.isalpha()]
    if len(letters) == 0:
        return 0.0
    return sum(1 for c in letters if c.isupper()) / len(letters)

df["caps_ratio"] = df["review_text"].apply(caps_ratio)

# 5) Basic "has URL" flag
url_pattern = re.compile(r"http[s]?://|www\.")
df["has_url"] = df["review_text"].astype(str).apply(lambda x: bool(url_pattern.search(x)))

# 6) Punctuation intensity (simple proxy for tone)
def exclam_count(text):
    return str(text).count("!")

def question_count(text):
    return str(text).count("?")

df["exclam_count"] = df["review_text"].apply(exclam_count)
df["question_count"] = df["review_text"].apply(question_count)

# 7) Language detection (safe)
def detect_language_safe(text):
    try:
        t = str(text).strip()
        if len(t) < 5:   # too short often fails
            return "unknown"
        return detect(t)
    except LangDetectException:
        return "unknown"

df["language"] = df["review_text"].apply(detect_language_safe)

# 8) Simple "is mostly non-text" flag (emoji-only / very low alphabetic content)
def alpha_ratio(text):
    text = str(text)
    if len(text) == 0:
        return 0.0
    alpha = sum(1 for c in text if c.isalpha())
    return alpha / len(text)

df["alpha_ratio"] = df["review_text"].apply(alpha_ratio)
df["low_text_content"] = df["alpha_ratio"] < 0.20  # tweakable threshold

print("\n✅ Feature engineering complete.")
print(df[[
    "review_text",
    "review_char_len",
    "review_word_count",
    "emoji_count",
    "caps_ratio",
    "has_url",
    "exclam_count",
    "question_count",
    "language",
    "low_text_content"
]].head(5))

print("\n=== Feature Summary Statistics ===")
print(df[[
    "review_char_len",
    "review_word_count",
    "emoji_count",
    "caps_ratio",
    "exclam_count",
    "question_count"
]].describe())

print(df["language"].value_counts())
print(df["has_emoji"].mean())
print(df["low_text_content"].mean())

df.to_csv("full_feature_engineering_table.csv", index=False)
print("Saved full feature table to CSV.")

metrics_df = pd.DataFrame({
    "Accuracy": [accuracy_score(y_true, y_pred)],
    "Macro_F1": [f1_score(y_true, y_pred, average="macro")]
})

metrics_df.to_csv("model_performance.csv", index=False)
print("Saved: model_performance.csv")

labels = ["negative", "neutral", "positive"]
cm = confusion_matrix(y_true, y_pred, labels=labels)

cm_df = pd.DataFrame(cm, index=labels, columns=labels)
cm_df.to_csv("confusion_matrix.csv")
print("Saved: confusion_matrix.csv")

plt.figure(figsize=(6,5))
sns.heatmap(cm_df, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.ylabel("True Label")
plt.xlabel("Predicted Label")
plt.tight_layout()
plt.savefig("confusion_matrix.png")
plt.close()
print("Saved: confusion_matrix.png")

language_acc = df.groupby("language").agg(
    n=("correct", "size"),
    accuracy=("correct", "mean")
).sort_values("n", ascending=False)

language_acc.to_csv("accuracy_by_language.csv")
print("Saved: accuracy_by_language.csv")

emoji_acc = df.groupby("has_emoji").agg(
    n=("correct", "size"),
    accuracy=("correct", "mean")
)

emoji_acc.to_csv("accuracy_by_emoji.csv")
print("Saved: accuracy_by_emoji.csv")

df["len_bin"] = pd.qcut(df["review_char_len"], 4, duplicates="drop")

length_acc = df.groupby("len_bin").agg(
    n=("correct", "size"),
    accuracy=("correct", "mean")
)

length_acc.to_csv("accuracy_by_length.csv")
print("Saved: accuracy_by_length.csv")

df["caps_bin"] = pd.qcut(df["caps_ratio"], 4, duplicates="drop")

caps_acc = df.groupby("caps_bin").agg(
    n=("correct", "size"),
    accuracy=("correct", "mean")
)

caps_acc.to_csv("accuracy_by_caps_ratio.csv")
print("Saved: accuracy_by_caps_ratio.csv")

low_text_acc = df.groupby("low_text_content").agg(
    n=("correct", "size"),
    accuracy=("correct", "mean")
)

low_text_acc.to_csv("accuracy_by_low_text_content.csv")
print("Saved: accuracy_by_low_text_content.csv")

